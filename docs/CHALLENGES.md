# Challenges and Solutions

This document outlines the key challenges encountered during the entity resolution project and how they were successfully addressed.

## 1. Memory Limitations

### Challenge

The initial approach attempted to create a full similarity matrix comparing all pairs of records:

```python
n = 33,446  # number of records
similarity_matrix = np.zeros((n, n))  # creates nÃ—n matrix

```

This resulted in an out-of-memory error:

```
numpy._core._exceptions._ArrayMemoryError: Unable to allocate 8.33 GiB for an array with shape (33446, 33446) and data type float64

```

The full matrix would require approximately 8.33 GB of RAM, which exceeded the available memory on the standard laptop used for development.

### Solution

Implemented a memory-efficient blocking strategy that:

1. **Groups similar records into blocks** based on name prefix, domain, phone, and location
2. **Compares only within blocks** rather than all-pairs comparison
3. **Stores only similarity values above threshold** instead of the full matrix

```python
# Instead of full matrix
similar_pairs = []  # stores only (i, j, similarity) tuples for similar pairs

# Process each block separately
for block_type, block_key, indices in blocks:
    for i in range(len(indices)):
        for j in range(i+1, len(indices)):
            # Calculate similarity only for records in the same block
            similarity = calculate_similarity(indices[i], indices[j])
            if similarity >= threshold:
                similar_pairs.append((indices[i], indices[j], similarity))

```

**Result**: Memory usage reduced from 8.33 GB to less than 1 GB, enabling the solution to run on standard hardware.

## 2. Balancing Precision and Recall

### Challenge

Setting the similarity threshold too high would miss many true duplicates (low recall), while setting it too low would incorrectly merge distinct companies (low precision).

### Solution

Implemented a multi-faceted approach:

1. **Weighted multi-attribute similarity**:
    - Assigned different weights to different attributes (name: 35%, domain: 25%, phone: 15%, location: 15%, industry: 10%)
    - This allows records to match even if some attributes are missing or different
2. **Empirical threshold tuning**:
    - Tested different thresholds (0.6, 0.7, 0.8) on sample data
    - Selected 0.7 as the optimal balance between precision and recall
3. **Multiple blocking strategies**:
    - Implemented 4 complementary blocking methods
    - Ensures records have multiple chances to be compared, increasing recall

**Result**: Achieved high accuracy in matching, with manual inspection confirming both high precision and recall.

## 3. Handling Missing Data

### Challenge

Many records had incomplete information:

- 80% missing legal names
- 50% missing phone numbers
- 30% missing geographic coordinates
- 20% missing website domains

This made it difficult to reliably match records, as key fields for comparison were often unavailable.

### Solution

Designed a robust matching approach that:

1. **Uses multiple attributes** so records can still match even if some fields are missing
2. **Implements fallback strategies** for each attribute:
    - No coordinates? Use address text similarity
    - No domain? Try phone and name similarity
    - No phone? Rely more on name and location
    

**Result**: Successfully matched records even with significant missing data, achieving a 80.29% duplication rate.

## 4. Handling Name Variations

### Challenge

Companies appeared with many naming variations:

- Different legal forms: "ABC Inc" vs "ABC Corporation" vs "ABC LLC"
- Word order differences: "New York Coffee Shop" vs "Coffee Shop New York"
- Abbreviations: "Intl" vs "International"
- Location inclusion: "Starbucks Seattle" vs "Starbucks"
- Spelling variations and typos: "Color" vs "Colour", "McDonalds" vs "McDonald's"

### Solution

Implemented comprehensive name normalization and matching:

1. **Name normalization pipeline**:
    
    ```python
    def _normalize_company_name(self, name):
        if name is None:
            return ''
    
        # Convert to lowercase
        name = name.lower()
    
        # Remove legal entity types
        legal_forms = [
            'llc', 'ltd', 'inc', 'corp', 'corporation', 'gmbh', 'limited',
            # ... many more legal forms
        ]
    
        for form in legal_forms:
            pattern = r'\b' + re.escape(form) + r'\b\.?'
            name = re.sub(pattern, '', name)
    
        # Replace special characters with space
        name = re.sub(r'[^\w\s]', ' ', name)
    
        # Replace multiple spaces with a single space
        name = re.sub(r'\s+', ' ', name)
    
        # Remove standalone digits
        name = re.sub(r'\b\d+\b', '', name)
    
        return name.strip()
    
    ```
    
2. **Fuzzy string matching** using Jaro-Winkler similarity:
    - Handles transpositions and common misspellings
    - Gives more weight to characters that match at the beginning
3. **Cross-matching with commercial names**:
    - Compare primary name with all commercial name variations
    - Compare all commercial names with each other

**Result**: Successfully matched company names despite significant variations in representation.

## 5. Scaling to Large Dataset

### Challenge

Processing 33,446 records with all the necessary comparisons required efficient algorithms and data structures to complete in reasonable time.

### Solution

Implemented several performance optimizations:

1. **Optimized blocking strategy**:
    - Limited very large blocks (>1000 records) with additional filters
    - For large blocks, only compared companies with similar name lengths
2. **Early termination in similarity calculation**:
    - If attributes with high weights (name, domain) don't match at all, skip further comparison
3. **Memory-efficient data structures**:
    - Used NetworkX sparse graph representation
    - Stored only similar pairs, not full similarity matrix
4. **Progress monitoring**:
    - Added progress bars with tqdm to track processing of blocks
    - Added detailed logging to monitor each step of the pipeline

**Result**: Processed the entire dataset in approximately 10 minutes on a standard laptop.

## 6. Representative Record Selection

### Challenge

After identifying clusters of duplicate records, needed to select the most complete and accurate record as the representative for each unique company.

### Solution

Implemented a scoring system that:

1. **Evaluates completeness** of each record:
    
    ```python
    # Calculate completeness score for each record
    completeness_scores = {}
    for idx, record in cluster_records.iterrows():
        score = 0
        for i, col in enumerate(key_columns):
            # Weight earlier columns more heavily
            weight = len(key_columns) - i
    
            # Check if value exists and is not None
            if not pd.isna(record[col]) and record[col] != 'None' and record[col] != '':
                score += weight
    
    ```
    
2. **Prioritizes key fields** with higher weights:
    - Company name, website, and phone valued more than secondary fields
3. **Considers recency** of records:
    
    ```python
    # Bonus for newer records
    if 'last_updated_at' in record and record['last_updated_at'] is not None:
        try:
            # Extract year from timestamp
            year = int(record['last_updated_at'].split('-')[0])
            # Add bonus for newer records (0-3 points)
            if year >= 2020:
                score += (year - 2020) + 1
        except:
            pass
    
    ```
    

**Result**: Selected the most informative record for each unique company, creating a high-quality deduplicated dataset.

## 7. Output Format Design

### Challenge

Needed to design an output format that would:

- Preserve all original data
- Clearly identify clusters of duplicates
- Indicate representative records
- Be easily usable for downstream applications

### Solution

Created a two-part output strategy:

1. **Complete results file** with all records and additional fields:
    - `cluster_id`: Unique identifier for each cluster
    - `is_representative`: Boolean flag for representative records
    - `duplicate_ids`: List of IDs for non-representative duplicates
    - `cluster_size`: Number of records in the cluster
2. **Deduplicated results file** with only representative records:
    - Contains one record per unique company
    - Includes cluster information for reference back to full dataset

**Result**: Clean, well-structured output that clearly identifies unique companies and their duplicates, ready for further use or analysis.

## Conclusion

The challenges encountered during this project required creative problem-solving and algorithmic thinking. By addressing the memory limitations through blocking strategies, handling data quality issues with robust matching approaches, and optimizing for performance, I was able to create a solution that successfully processed 33,446 records and identified 6,591 unique companies with a duplication rate of 80.29%.

The memory-efficient design was particularly crucial, reducing requirements from 8.33 GB to less than 1 GB, which enabled the solution to run on standard hardware without compromising on accuracy or completeness.
